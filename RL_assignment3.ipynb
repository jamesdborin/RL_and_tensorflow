{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_assignment3.ipynb",
      "provenance": [
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_3_solution.ipynb?workspaceId=hado:ucl_2020::citc",
          "timestamp": 1585768145675
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_3_solution.ipynb",
          "timestamp": 1585065133496
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_3_solution.ipynb",
          "timestamp": 1584636565210
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2019/RL_assignment_3_solution.ipynb",
          "timestamp": 1583489200868
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2019/RL_assignment_3_solution.ipynb",
          "timestamp": 1558729821395
        },
        {
          "file_id": "1-QHIC6wblgNWUTmb8xuIqsFg_aHccWcD",
          "timestamp": 1551446679353
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2019/RL_assignment_3.ipynb?workspaceId=mtthss:ucl::citc",
          "timestamp": 1551279480610
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2018/RL_assignment_3.ipynb?workspaceId=mtthss:ucl::citc",
          "timestamp": 1551102549971
        },
        {
          "file_id": "1KT0T0MXgoBAx-GxR8A0shFRGjCqmU2aR",
          "timestamp": 1542801476444
        },
        {
          "file_id": "1mIWeHr4YB6AVcwtRszH87Lj7y3pEhLVM",
          "timestamp": 1520447403512
        },
        {
          "file_id": "1AmoBDF9hWeKfxWp6h_ihyI-KFbhQeSJW",
          "timestamp": 1520259410401
        },
        {
          "file_id": "1FwMxkDPkt68fxovrMmmWwm6ohYvX2wt1",
          "timestamp": 1517660129183
        },
        {
          "file_id": "1wwTq5nociiMHUb26jxrvZvGN6l11xV5o",
          "timestamp": 1517174839485
        },
        {
          "file_id": "1_gJNoj9wG4mnigscGRAcZx7RHix3HCjG",
          "timestamp": 1515086437469
        },
        {
          "file_id": "1hcBeMVfaSh8g1R2ujtmxOSHoxJ8xYkaW",
          "timestamp": 1511098107887
        }
      ],
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL homework 3\n",
        "\n",
        "-------------------------------\n",
        "\n",
        "\n",
        "**Name:** Your Name\n",
        "\n",
        "**SN:** Your Student Number\n",
        "\n",
        "-----------------------------------\n",
        "\n",
        "\n",
        "**Due date:** *May 13, 2020, 11:00 am*\n",
        "\n",
        "------------------------------------\n",
        "\n",
        "## How to Submit\n",
        "\n",
        "When you have completed the exercises and everything has finsihed running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **studentnumber_RL_hw3.ipynb** before the deadline above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rNuohp44N00i"
      },
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "#### Part 1:\n",
        "You will implement several algorithms to investigate the role of planning in reinforcement learning.\n",
        "\n",
        "#### Part 2:\n",
        "You will be guided through the implementation of a full deep reinforcement learning agent.\n",
        "\n",
        "#### Part 3:\n",
        "You will implement a number of off-policy multi-step return estimates, and answer questions about their accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nVBcO5mAV9Ow"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Run all the cells in this section, but do not modify them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z1p0fpbxQLyn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ps5OnkPmDbMX",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eFnvhnKlWN_Z"
      },
      "source": [
        "## Gridworlds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "e5VkDWDTWNHE",
        "colab": {}
      },
      "source": [
        "#@title Implementation\n",
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, discount=0.9):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = np.array([\n",
        "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1,  0,  0, 10, -1],\n",
        "      [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
        "    ])\n",
        "    self._start_state = (2, 2)\n",
        "    self._goal_state = (8, 2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._discount = discount\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "      return self._number_of_states\n",
        "    \n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(self._layout > -1, interpolation=\"nearest\", cmap=\"YlOrRd_r\")     \n",
        "    ax = plt.gca()\n",
        "    ax.grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(\n",
        "        self._start_state[0], self._start_state[1], \n",
        "        r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(\n",
        "        self._goal_state[0], self._goal_state[1], \n",
        "        r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-3):\n",
        "      plt.plot([+0.5, w-1.5], [y+1.5, y+1.5], '-k', lw=2, alpha=0.5)\n",
        "    for x in range(w-3):\n",
        "      plt.plot([x+1.5, x+1.5], [+0.5, h-1.5], '-k', lw=2, alpha=0.5)\n",
        "\n",
        "  \n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "  \n",
        "  def int_to_state(self, int_obs):\n",
        "    x = int_obs % self._layout.shape[1]\n",
        "    y = int_obs // self._layout.shape[1]\n",
        "    return y, x\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "\n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    if self._layout[new_y, new_x] == -1:  # wall\n",
        "      reward = -5.\n",
        "      discount = self._discount\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = 0.\n",
        "      discount = self._discount\n",
        "    else:  # a goal\n",
        "      reward = self._layout[new_y, new_x]\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "    \n",
        "    self._state = new_state\n",
        "    return reward, discount, self.get_obs()\n",
        "  \n",
        "class AltGrid(Grid):\n",
        "  \n",
        "    def __init__(self, discount=0.9):\n",
        "      # -1: wall\n",
        "      # 0: empty, episode continues\n",
        "      # other: number indicates reward, episode will terminate\n",
        "      self._layout = np.array([\n",
        "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
        "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1,  0, 10,  0,  0,  0,  0,  0,  0, -1],\n",
        "        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
        "      ])\n",
        "      self._start_state = (2, 2)\n",
        "      self._goal_state = (2, 7)\n",
        "      self._state = self._start_state\n",
        "      self._number_of_states = np.prod(np.shape(self._layout))\n",
        "      self._discount = discount\n",
        "\n",
        "class FeatureGrid(Grid):\n",
        "  \n",
        "  def get_obs(self):\n",
        "    return self.state_to_features(self._state)\n",
        "  \n",
        "  def state_to_features(self, state):\n",
        "    y, x = state\n",
        "    x /= float(self._layout.shape[1] - 1)\n",
        "    y /= float(self._layout.shape[0] - 1)\n",
        "    markers = np.arange(0.1, 1.0, 0.1)\n",
        "    features = np.array([np.exp(-40*((x - m)**2+(y - n)**2))\n",
        "                         for m in markers\n",
        "                         for n in markers] + [1.])\n",
        "    return features / np.sum(features**2)\n",
        "  \n",
        "  def int_to_features(self, int_state):\n",
        "    return self.state_to_features(self.int_to_state(int_state))\n",
        "  \n",
        "  @property\n",
        "  def number_of_features(self):\n",
        "      return len(self.get_obs())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "zV0NxnIyWVtu",
        "colab": {}
      },
      "source": [
        "#@title Show gridworlds\n",
        "\n",
        "# Plot tabular environments\n",
        "grid = Grid()\n",
        "alt_grid = AltGrid()\n",
        "print(\"A grid world\")\n",
        "grid.plot_grid()\n",
        "plt.show()\n",
        "print(\"\\nAn alternative grid world\")\n",
        "alt_grid.plot_grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot features of each state for non tabular version of the environment.\n",
        "print(\n",
        "    \"\\nFeatures (visualised as 9x9 heatmaps) for different locations in the grid\"\n",
        "    \"\\n(Note: includes unreachable states that coincide with walls in this visualisation.)\"\n",
        ")\n",
        "feat_grid = FeatureGrid()\n",
        "shape = feat_grid._layout.shape\n",
        "f, axes = plt.subplots(shape[0], shape[1])\n",
        "for state_idx, ax in enumerate(axes.flatten()):\n",
        "  ax.imshow(np.reshape((feat_grid.int_to_features(state_idx)[:-1]),(9,9)),\n",
        "            interpolation='nearest',\n",
        "            cmap='plasma')\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3lpweIqAWBX3"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "95Ly2AaMWA5e",
        "colab": {}
      },
      "source": [
        "#@title helper functions (run, but don't modify this cell)\n",
        "\n",
        "def run_experiment(env, agent, number_of_steps):\n",
        "    mean_reward = 0.\n",
        "    try:\n",
        "      action = agent.initial_action()\n",
        "    except AttributeError:\n",
        "      action = 0\n",
        "    for i in range(number_of_steps):\n",
        "      reward, discount, next_state = env.step(action)\n",
        "      action = agent.step(reward, discount, next_state)\n",
        "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
        "\n",
        "    return mean_reward\n",
        "  \n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_rewards(xs, rewards, color):\n",
        "  mean = np.mean(rewards, axis=0)\n",
        "  p90 = np.percentile(rewards, 90, axis=0)\n",
        "  p10 = np.percentile(rewards, 10, axis=0)\n",
        "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
        "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
        "\n",
        "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\n",
        "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_state_value(action_values):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "\n",
        "def plot_action_values(action_values):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  vmin = np.min(action_values)\n",
        "  vmax = np.max(action_values)\n",
        "  dif = vmax - vmin\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
        "    \n",
        "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "    \n",
        "  plt.subplot(3, 3, 5)\n",
        "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
        "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "\n",
        "def parameter_study(parameter_values, parameter_name,\n",
        "  agent_constructor, env_constructor, color, repetitions=10, number_of_steps=int(1e4)):\n",
        "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  for rep in range(repetitions):\n",
        "    for i, p in enumerate(parameter_values):\n",
        "      env = env_constructor()\n",
        "      agent = agent_constructor()\n",
        "      if 'eps' in parameter_name:\n",
        "        agent.set_epsilon(p)\n",
        "      elif 'alpha' in parameter_name:\n",
        "        agent._step_size = p\n",
        "      else:\n",
        "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
        "      mean_rewards[rep, i] = run_experiment(grid, agent, number_of_steps)\n",
        "      agent.set_epsilon(0.)\n",
        "      agent._step_size = 0.\n",
        "      greedy_rewards[rep, i] = run_experiment(grid, agent, number_of_steps//10)\n",
        "      del env\n",
        "      del agent\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plot_rewards(parameter_values, mean_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plot_rewards(parameter_values, greedy_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "  \n",
        "def random_policy(q):\n",
        "  return np.random.randint(4)\n",
        "\n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])\n",
        "\n",
        "def plot_greedy_policy(grid, q):\n",
        "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
        "  greedy_actions = np.argmax(q, axis=2)\n",
        "  grid.plot_grid()\n",
        "  for i in range(9):\n",
        "    for j in range(10):\n",
        "      action_name = action_names[greedy_actions[i,j]]\n",
        "      plt.text(j, i, action_name, ha='center', va='center')\n",
        "\n",
        "def plot_greedy_policy_v2(grid, pi):\n",
        "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
        "  greedy_actions = np.argmax(pi, axis=2)\n",
        "  grid.plot_grid()\n",
        "  h, w = grid._layout.shape\n",
        "  for y in range(2, h-2):\n",
        "    for x in range(2, w-2):\n",
        "      action_name = action_names[greedy_actions[y-2, x-2]]\n",
        "      plt.text(x, y, action_name, ha='center', va='center')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sC3M-IfEq2bI"
      },
      "source": [
        "# Part 1: Planning [50 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nxVB3cCccYO-"
      },
      "source": [
        "## 1.1: Implement Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j37IiBzpcdB4"
      },
      "source": [
        "### Question 1.1.1\n",
        "**[3 pts]** \n",
        "\n",
        "Implement a trainable **tabular model** of the environment.\n",
        "\n",
        "The model should implement: \n",
        "* a *next_state* method, taking a state and action and returning the next state in the environment.\n",
        "* a *reward* method, taking a state and action and returning the immediate reward associated to executing that action in that state.\n",
        "* a *discount* method, taking a state and action and returning the discount associated to executing that action in that state.\n",
        "* a *transition* method, taking a state and an action and returning both the next state and the reward associated to that transition.\n",
        "* a *update* method, taking a full transition *(state, action, reward, next_state)* and updating the model predictions.\n",
        "\n",
        "Given that the environment is deterministic and tabular the model will reduce to a simple lookup table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qzvPua0Sca2o",
        "colab": {}
      },
      "source": [
        "class TabularModel(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions):\n",
        "    pass\n",
        "\n",
        "  def next_state(self, s, a):\n",
        "    pass\n",
        "  \n",
        "  def reward(self, s, a):\n",
        "    pass\n",
        "\n",
        "  def discount(self, s, a):\n",
        "    pass\n",
        "  \n",
        "  def transition(self, state, action):\n",
        "    return (\n",
        "        self.reward(state, action), \n",
        "        self.discount(state, action),\n",
        "        self.next_state(state, action))\n",
        "  \n",
        "  def update(self, state, action, reward, discount, next_state):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N6p8hNTLcyAK"
      },
      "source": [
        "### Question 1.1.2\n",
        "\n",
        "**[5 pts]** \n",
        "\n",
        "Implement a trainable **linear model** of the environment.\n",
        "\n",
        "The Model should implement: \n",
        "* a *next_state* method, taking a state and action and returning the predicted next state in the environment.\n",
        "* a *reward* method, taking a state and action and returning the predicted immediate reward associated to executing that action in that state.\n",
        "* a *discount* method, taking a state and action and returning the predicted discount associated to executing that action in that state.\n",
        "* a *transition* method, taking a state and an action and returning both the next state and the reward associated to that transition.\n",
        "* a *update* method, taking a full transition *(state, action, reward, next_state)* and updating the model (in its reward, discount and next_state component)\n",
        "\n",
        "For each selected action, the predicted reward $r'$, discount $\\gamma'$ and next state $\\mathbf{s}'$ will all be a linear function of the current state $\\mathbf{s}$.\n",
        "* $\\mathbf{s}' = \\mathbf{M}_a \\mathbf{s}$\n",
        "* $r' = (\\mathbf{m}^r_a)^{\\top} \\mathbf{s}$\n",
        "* $\\gamma' = (\\mathbf{m}^{\\gamma}_a)^{\\top} \\mathbf{s}$\n",
        "\n",
        "Where $\\mathbf{M}_a$ is a matrix of shape $(\\text{number_of_features}, \\text{number_of_features})$, $\\mathbf{m}^r_a$ and $\\mathbf{m}^{\\gamma}_a$ are vectors of shape $(\\text{number_of_features},)$\n",
        "\n",
        "The parameters of all these linear transformations must be trained by stochastic gradient descent. \n",
        "\n",
        "Write down the update to the parameters of the models and implement the update in the model below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ab--TR-5c98N",
        "colab": {}
      },
      "source": [
        "class LinearModel(object):\n",
        "\n",
        "  def __init__(self, number_of_features, number_of_actions):\n",
        "    pass\n",
        "\n",
        "  def next_state(self, s, a):\n",
        "    pass\n",
        "  \n",
        "  def reward(self, s, a):\n",
        "    pass\n",
        "\n",
        "  def discount(self, s, a):\n",
        "    pass\n",
        "\n",
        "  def transition(self, state, action):\n",
        "    return (\n",
        "        self.reward(state, action),\n",
        "        self.discount(state, action),\n",
        "        self.next_state(state, action))\n",
        "\n",
        "  def update(self, state, action, reward, discount, next_state, step_size=0.1):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fzpb_dGVjT0O"
      },
      "source": [
        "## 1.2: Implement Agents\n",
        "\n",
        "We are going to implement 4 agent:\n",
        "- Tabular Q-learning with Experience Replay\n",
        "- Tabular Dyna-Q with a tabular model\n",
        "- Linear Q-learning with Experience Replay\n",
        "- Linear Dyna-Q with a linear model\n",
        "\n",
        "All agents you implement in this section must share the agent interface below:\n",
        "\n",
        "#### `__init__(self, number_of_actions, number_of_states, initial_observation)`:\n",
        "The constructor will provide the agent the number of actions, number of states, and the initial observation. You can get such initial observation by instatiating an environment (e.g., `grid = Grid()`), and then calling `grid.get_obs()`.\n",
        "\n",
        "#### `step(self, reward, discount, next_observation)`:\n",
        "The step should update the internal values, and return a new action to take. When the discount is zero ($\\text{discount} = \\gamma = 0$), the `next_observation` will be the initial observation of the next episode.  One shouldn't bootstrap on the value of this state, which can simply be guaranteed when using \"$\\gamma \\cdot v(\\text{next_observation})$\" in the update, because $\\gamma = 0$.  So, the end of an episode can be seamlessly handled with the same step function. Note that to perform updates within the `step` function you typically need to store the previous state and/or action: you may set such previous action to 0 in the constructor for consumption in the first step of the first episode of the agent's lifetime.\n",
        " \n",
        "#### `q_values()`:\n",
        "For tabular agents **only**. This method must return a matrix of Q values of shape: (`number_of_states`, `number_of_actions`)\n",
        "\n",
        "#### `q_values(state)`:\n",
        "For agents with function approximation **only**. This method must return an array of Q values of shape: (`number_of_actions`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "omzJxb5ds0Iq"
      },
      "source": [
        "### Question 1.2.1\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Implement an agent that uses **Experience Replay** to learn action values, at each step:\n",
        "* select actions randomly\n",
        "* accumulate each observed transition *(s, a, r, s')* in a *replay buffer*,\n",
        "* apply an online Q-learning update with the most recently observed transition,\n",
        "* apply multiple Q-learning updates based on transitions sampled (uniformly) from the *replay buffer* (in addition to the online updates).\n",
        "\n",
        "So, the `step` function of the agent will, conceptually, look as follows:\n",
        "\n",
        "1. Append most recent observed transition $(S_t, A_t, R_{t+1}, \\gamma, S_{t+1})$ to replay buffer\n",
        "\n",
        "1. Update values: $Q(S_t, A_t)$ with Q-learning, using transition $(S_t, A_t, R_{t+1}, \\gamma, S_{t+1})$ \n",
        "\n",
        "1. Loop repeat n times:\n",
        "\n",
        "  1. Sample $S, A, R, \\gamma, S'$ from replay\n",
        "  \n",
        "  1. Update values: $Q(S, A)$ with Q-learning, using transition $(S, A, R, \\gamma, S')$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TB9e_reb2pJX",
        "colab": {}
      },
      "source": [
        " class ExperienceQ(object):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, initial_state, \n",
        "      behaviour_policy, num_offline_updates=0, step_size=0.1):\n",
        "    pass\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    pass\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MKfA7ifHvO-M"
      },
      "source": [
        "\n",
        "### Question 1.2.2\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Implement an agent that uses **Dyna-Q** to learn action values.\n",
        "* select actions randomly\n",
        "* accumulate all observed transitions *(s, a, r, s')* in the environment in a *replay buffer*,\n",
        "* apply an online Q-learning to Q-value\n",
        "* apply an update to the *model* based on the latest transition\n",
        "* apply multiple Q-learning updates based on transitions *(s, a, model.reward(s), model.next_state(s))* for some previous state and action pair *(s, a)*.\n",
        "\n",
        "So, the `step` function conceptually looks as follows:\n",
        "1. Append most recent observed transition $(S_t, A_t, R_{t+1}, \\gamma, S_{t+1})$ to replay buffer\n",
        "\n",
        "1. Update values: $Q(S_t, A_t)$ with Q-learning, using transition $(S_t, A_t, R_{t+1}, \\gamma, S_{t+1})$\n",
        "\n",
        "1. Update model: $M(S_t, A_t)$, using transition $(S_t, A_t, R_{t+1}, \\gamma, S_{t+1})$\n",
        "\n",
        "1. Loop repeat n times:\n",
        "\n",
        "  1. Sample $S, A$ from replay\n",
        "  \n",
        "  1. Generate $R, S' = M(S, A)$\n",
        "  \n",
        "  1. Update values: $Q(S, A)$ with Q-learning, using transition $(S, A, R, \\gamma, S')$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WdJgVK6_3Q3-",
        "colab": {}
      },
      "source": [
        "class DynaQ(object):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_states, number_of_actions, initial_state, \n",
        "      behaviour_policy, num_offline_updates=0, step_size=0.1):\n",
        "    pass\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    pass\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ra01mmV5VPgm"
      },
      "source": [
        "### Question 1.2.3\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Implement an agent that uses **Experience Replay** to learn action values as a **linear function approximation** over a given set of features.\n",
        "\n",
        "Learn the value estimates via online stochastic gradient descent.  The observed `state` will now be a vector of length `number_of_features`, and the value estimate will be $w_t^\\top x_t$, where $w_t$ are the current weights and $x_t$ is the observed (agent) state at time $t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XOy_bpVa3j6V",
        "colab": {}
      },
      "source": [
        "class LinearExperienceQ(ExperienceQ):\n",
        "\n",
        "  def __init__(\n",
        "      self, number_of_features, number_of_actions, *args, **kwargs):\n",
        "    super(LinearExperienceQ, self).__init__(\n",
        "        number_of_actions=number_of_actions, *args, **kwargs)\n",
        "    pass\n",
        "\n",
        "  def q(self, state):\n",
        "    pass\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hlu3YPGAO9ss"
      },
      "source": [
        "### Question 1.2.4\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Implement an agent that uses **Dyna-Q** that uses a **linear function approximation** to represent values as well as for the model of the environment.\n",
        "\n",
        "Represent and learn both the **transition model** and the **reward model** as linear, action-dependent transformations of the given set of features.  The transition and reward models should be represented separately.  Implement separate models for each action (instead of, e.g., passing a one-hot identifier of the action in).\n",
        "\n",
        "Learn value estimates, transition model and reward model via online stochastic gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1RxFwgIU39dI",
        "colab": {}
      },
      "source": [
        "class LinearDynaQ(DynaQ):\n",
        "\n",
        "  def __init__(self, number_of_features, number_of_actions, *args, **kwargs):\n",
        "    super(LinearDynaQ, self).__init__(\n",
        "        number_of_actions=number_of_actions, *args, **kwargs)\n",
        "    pass\n",
        "\n",
        "  def q(self, state):\n",
        "    pass\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    g = discount\n",
        "    next_s = next_state\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1jZsPzCmDxAh"
      },
      "source": [
        "## 1.3: Analyse Results\n",
        "\n",
        "You will have to analyse experiments that evaluate each of these 5 agents in various settings, and in terms of different metrics.\n",
        "\n",
        "- Tabular learning: data efficiency\n",
        "- Tabular learning: computational efficiency\n",
        "- Linear function approximation\n",
        "- Learning in non-stationary environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qekcmj4R5Y6J"
      },
      "source": [
        "### Run data efficiency experiments\n",
        "\n",
        "*Online Q-learning*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $2000$ and $\\text{num_offline_updates}$ = $0$\n",
        "\n",
        "*Experience Replay*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $2000$ and $\\text{num_offline_updates}$ = $30$\n",
        "\n",
        "*DynaQ*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $2000$ and $\\text{num_offline_updates}$ = $30$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Iix-yw-MKS4Y",
        "colab": {}
      },
      "source": [
        "# Online Q\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=0, step_size=0.1)\n",
        "run_experiment(grid, agent, int(2e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ASml5uAeIl4A",
        "colab": {}
      },
      "source": [
        "# Experience Replay\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(2e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YG-_cjw-wRzm",
        "colab": {}
      },
      "source": [
        "# DynaQ\n",
        "grid = Grid()\n",
        "agent = DynaQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(2e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "teXnSHqjGfoT"
      },
      "source": [
        "### Question 1.3.1\n",
        "\n",
        "**[3 pts]**\n",
        "\n",
        "In the experiments above, how do the learnt value estimates differ between the online Q-learning, ExperienceReplay, and Dyna Q agents?\n",
        "\n",
        "Explain meaningful differences in at most 5 sentences.\n",
        "\n",
        "> *answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ujZNsXFY52fi"
      },
      "source": [
        "### Run experiments matching computational cost\n",
        "\n",
        "*Online Q-learning*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $62,000$ and $\\text{num_offline_updates}$ = $0$\n",
        "\n",
        "*ExperienceReplay*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $2000$ and $\\text{num_offline_updates}$ = $30$\n",
        "\n",
        "*DynaQ*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $2000$ and $\\text{num_offline_updates}$ = $30$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OVVWtGoUwiAe",
        "colab": {}
      },
      "source": [
        "# OnlineQ\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=0, step_size=0.1)\n",
        "run_experiment(grid, agent, int(62e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PuoUs8xVxady",
        "colab": {}
      },
      "source": [
        "# Experience Replay\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(2e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hDOW4dd221L6",
        "colab": {}
      },
      "source": [
        "# DynaQ\n",
        "grid = Grid()\n",
        "agent = DynaQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(2e3))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "plot_action_values(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nlFwZeKjLFEq"
      },
      "source": [
        "What if sampling from the environment is cheap and we don't care about data efficiency but only care about the amount of computation we use? \n",
        "\n",
        "The experiments directly above are the same as those above question 1.3.1, except that we ran the experiments for the same number of **total updates**, rather than the same number of **steps in the environment**, therefore using more data for the online Q-learning algorithm which *only* updates from real data.\n",
        "\n",
        "### Question 1.3.2\n",
        "\n",
        "**[3 pts]**\n",
        "\n",
        "How do the learnt values, and the relative performances, change, compared to the experiment above question 1.3.1?  Explain in at most 5 sentences.\n",
        "\n",
        "> *answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GBLluo2AXMsH"
      },
      "source": [
        "### Run experiments with linear function approximation\n",
        "\n",
        "We will now use the $\\text{FeatureGrid}$ domain, and consider the same 3 algorithms in the context of linear function approximation.\n",
        "\n",
        "*Online Q-learning*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $100,000$ and $\\text{num_offline_updates}$ = $0$\n",
        "\n",
        "*ExperienceReplay*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $100,000$ and $\\text{num_offline_updates}$ = $10$\n",
        "\n",
        "*DynaQ*\n",
        "\n",
        "* $\\text{number_of_steps}$ = $100,000$ and $\\text{num_offline_updates}$ = $10$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zwlRPm1uXMyv",
        "colab": {}
      },
      "source": [
        "# OnlineQ\n",
        "grid = FeatureGrid()\n",
        "\n",
        "agent = LinearExperienceQ(\n",
        "  number_of_features=grid.number_of_features, number_of_actions=4,\n",
        "  number_of_states=grid._layout.size, initial_state=grid.get_obs(),\n",
        "  num_offline_updates=0, step_size=0.01, behaviour_policy=random_policy)\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = np.reshape(\n",
        "    np.array([agent.q(grid.int_to_features(i))\n",
        "              for i in range(grid.number_of_states)]),\n",
        "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
        "plot_action_values(q)\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wb6XeKzXcIsi",
        "colab": {}
      },
      "source": [
        "# Experience Replay\n",
        "grid = FeatureGrid()\n",
        "\n",
        "agent = LinearExperienceQ(\n",
        "  number_of_features=grid.number_of_features, number_of_actions=4,\n",
        "  number_of_states=grid._layout.size, initial_state=grid.get_obs(),\n",
        "  num_offline_updates=10, step_size=0.01, behaviour_policy=random_policy)\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = np.reshape(\n",
        "    np.array([agent.q(grid.int_to_features(i))\n",
        "              for i in range(grid.number_of_states)]),\n",
        "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
        "plot_action_values(q)\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DVDRVknH1MXw",
        "colab": {}
      },
      "source": [
        "# DynaQ\n",
        "grid = FeatureGrid()\n",
        "\n",
        "agent = LinearDynaQ(\n",
        "  number_of_features=grid.number_of_features, \n",
        "  number_of_actions=4,\n",
        "  number_of_states=grid._layout.size, \n",
        "  initial_state=grid.get_obs(),\n",
        "  num_offline_updates=10, \n",
        "  step_size=0.01,\n",
        "  behaviour_policy=random_policy)\n",
        "\n",
        "run_experiment(grid, agent, int(1e5))\n",
        "q = np.reshape(\n",
        "    np.array([agent.q(grid.int_to_features(i))\n",
        "              for i in range(grid.number_of_states)]),\n",
        "    [grid._layout.shape[0], grid._layout.shape[1], 4])\n",
        "plot_action_values(q)\n",
        "plot_greedy_policy(grid, q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1ese1lc0yNFU"
      },
      "source": [
        "### Question 1.3.3\n",
        "\n",
        "**[3 pts]**\n",
        "\n",
        "How do the value estimates learnt with function approximation differ from those learnt in the tabular setting, as in the experiment above question 2.2.1?\n",
        "\n",
        "Explain the results in at most 5 sentences.\n",
        "\n",
        "> *answer here*\n",
        "\n",
        "### Question 1.3.4\n",
        "\n",
        "**[3 pts]**\n",
        "\n",
        "Inspect the policies derived by training agents with linear function approximation on `FeatureGrid'. \n",
        "\n",
        "How do they compare to the optimal policy?\n",
        "\n",
        "> *answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "arP0Nf0XUGrB"
      },
      "source": [
        "### Run experiments in a non stationary environments\n",
        "\n",
        "We now consider a non-stationary setting where after `pretrain_steps` in the environment, the goal is moved to a new location (from the top-right of the grid to the bottom-left). The agent is allowed to continue training for a (shorter) amount of time in this new setting, and then we evaluate the value estimates.\n",
        "\n",
        "*Online Q-learning*\n",
        "\n",
        "* $\\text{pretrain_steps}$ = $20,000$,  $\\text{num_steps}$ = $666$, and $\\text{num_offline_updates}$ = $0$\n",
        "\n",
        "*ExperienceReplay*\n",
        "\n",
        "* $\\text{pretrain_steps}$ = $20,000$,  $\\text{num_steps}$ = $666$ and $\\text{num_offline_updates}$ = $10$\n",
        "\n",
        "*DynaQ*\n",
        "\n",
        "* $\\text{pretrain_steps}$ = $20,000$,  $\\text{num_steps}$ = $666$ and $\\text{num_offline_updates}$ = $10$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6P9lC323X7uH",
        "colab": {}
      },
      "source": [
        "# Online Q\n",
        "\n",
        "# Train on first environment\n",
        "pretrain_steps = 2e4\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=0, step_size=0.1)\n",
        "run_experiment(grid, agent, int(pretrain_steps))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# Change goal location\n",
        "alt_grid = AltGrid()\n",
        "run_experiment(alt_grid, agent, int(pretrain_steps / 30))\n",
        "alt_q = agent.q_values.reshape(alt_grid._layout.shape + (4,))\n",
        "plot_state_value(alt_q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bq5msw1iY-Q5",
        "colab": {}
      },
      "source": [
        "# Experience Replay\n",
        "\n",
        "# Train on first environment\n",
        "pretrain_steps = 2e4\n",
        "grid = Grid()\n",
        "agent = ExperienceQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(pretrain_steps))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# Change goal location\n",
        "alt_grid = AltGrid()\n",
        "run_experiment(alt_grid, agent, int(pretrain_steps / 30))\n",
        "alt_q = agent.q_values.reshape(alt_grid._layout.shape + (4,))\n",
        "plot_state_value(alt_q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AwztU4EbUXe0",
        "colab": {}
      },
      "source": [
        "# DynaQ\n",
        "\n",
        "# Train on first environment\n",
        "pretrain_steps = 2e4\n",
        "grid = Grid()\n",
        "agent = DynaQ(\n",
        "  grid._layout.size, 4, grid.get_obs(),\n",
        "  random_policy, num_offline_updates=30, step_size=0.1)\n",
        "run_experiment(grid, agent, int(pretrain_steps))\n",
        "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
        "\n",
        "# Change goal location\n",
        "alt_grid = AltGrid()\n",
        "run_experiment(alt_grid, agent, int(pretrain_steps / 30))\n",
        "alt_q = agent.q_values.reshape(alt_grid._layout.shape + (4,))\n",
        "plot_state_value(alt_q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lc8tJPpXyNM7"
      },
      "source": [
        "### Question 1.3.5\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Compare the value estimates of online Q-learning and Dyna-Q, after training also on the new goal location.\n",
        "\n",
        "Explain what you see in at most 5 sentences. \n",
        "\n",
        "> *answer here*\n",
        "\n",
        "### Question 1.3.6\n",
        "\n",
        "**[5 pts]**\n",
        "\n",
        "Compare the value estimates of online Experience Replay and Dyna-Q, after training also on the new goal location, explain what you see.\n",
        "\n",
        "> *answer here*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7TYEJzjdBD8s"
      },
      "source": [
        "# Part 2: Deep RL [25 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iIRC73HLq6VH"
      },
      "source": [
        "### Actor-critics\n",
        "\n",
        "You are going to implement an Actor-critic agent that updates a policy parametrised as a deep neural network.\n",
        "\n",
        "The agent learns online from a single stream of experience, updating the parametes of its policy after each transition in the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KV03Q3MpveUM"
      },
      "source": [
        "### Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wc-kqp3tveUT",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/deepmind/bsuite.git\n",
        "!pip install bsuite/\n",
        "!pip install dm-haiku"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r_pTfi5dSFX5"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "huyKrYpvSHSu",
        "colab": {}
      },
      "source": [
        "from bsuite.experiments.catch import catch\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xtlrr5d2p7cS"
      },
      "source": [
        "### Neural networks\n",
        "\n",
        "You will now use JAX to define a network parametrising:\n",
        "\n",
        "* The values of each state $v_{\\pi}(s)$.\n",
        "* The action preferences in each state $\\{p_i(s)\\}_{i\\in\\{1, ..., |A|\\}}$ (you can think of and implement the preferences $\\mathbf{p}(s)$ as a vector output with $|A|$ elements).\n",
        "\n",
        "You will use `Haiku` (https://github.com/deepmind/dm-haiku) to define the network. You will need to:\n",
        "* define the forward pass of the network as some function `fn`\n",
        "* Use `hk.transform(fn)` to convert this in a pair of functions (`init_net`, and `apply_net`):\n",
        "\n",
        "The `init` function has signature `parameters = init_net(key, obs)`. \n",
        "  * takes a `jax.random.PRNGKey` and an `observation`\n",
        "  * returns randomly sampled weights for the neural network.\n",
        "\n",
        "The `apply_net` functions have signature `v, p = apply_net(parameters, obs)` \n",
        "  * takes the current set of `parameters` and an `observation`\n",
        "  * returns a scalar value `v` and a vector of preferences `p`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gj1nsUYuOoe1"
      },
      "source": [
        "### Q 3.1 [5 marks]\n",
        "\n",
        "Define the forward pass of the neural network. The network must:\n",
        "\n",
        "* take an `observation` as input\n",
        "* reshape the observation into a flat vector `flat_obs`\n",
        "* compute a hidden representation `h = Relu(W.dot(flat_obs) + b)`\n",
        "* compute a vector of action preferences as a linear function of `h`\n",
        "* compute a scalar state value as a linear function of `h`\n",
        "* return the scalar value and vector preferences.\n",
        "\n",
        "Note:\n",
        "* The hidden layer should have 50 units\n",
        "* the action preferences should be a vector of 3 elements (one per each available action),\n",
        "* the value should be a scalar (not a vector with one element).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vRBP0xjtQvTi",
        "colab": {}
      },
      "source": [
        "init_net, apply_net = ..., ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9ImJUgzFosvD"
      },
      "source": [
        "### Choosing actions\n",
        "\n",
        "A critical component of an actor-critic agent is a (stochastic) policy, mapping `observations` to `actions`. \n",
        "\n",
        "In deep RL, this mapping is conventionally parametrised by a deep neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1MVXNWwlYW24"
      },
      "source": [
        "### Q 3.2 [5 marks]\n",
        "\n",
        "Implement a softmax policy parametrised by the neural network above.\n",
        "\n",
        "The function has signature `action = policy(net_params, key, obs)`,\n",
        "* Taking the current network parameters `net_params`, a JAX random `key` and the current `observation`\n",
        "* Returning an `action` sampled from a softmax distribution (with temperature 1.) over the set of preferences output by the neural network.\n",
        "\n",
        "Functions to perform random sampling in JAX (e.g. those in `jax.random`) take a random key as input, and they are deterministic function of such a key. In general, in a JAX program you will therefore need to use the `split` function to generate a new random key before every new sampling. The run loop that runs the experiment later on takes care of this for you: you can assume that a new random `key` is provided to you on each call to the `policy`, and you do not need to split the key yourself within the function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x-FSOYNOYnUx",
        "colab": {}
      },
      "source": [
        "@jax.jit\n",
        "def policy(net_params, key, obs):\n",
        "  \"\"\"Sample action from a softmax policy.\"\"\"\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iMYcb7Y9krnT"
      },
      "source": [
        "### Learning values and policies\n",
        "\n",
        "An actor-critic agent requires to update the parameters of the network so as to simultaneously improve the value predictions and the policy.\n",
        "\n",
        "In the next section you will define the gradient updates for each of these two components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GsIlpmNEk5fv"
      },
      "source": [
        "### Q 3.3 [5 marks]\n",
        "\n",
        "Implement a function to compute a stochastic estimate of the policy gradient from a 1 step transition in the environment.\n",
        "\n",
        "* You will use $R_{t+1} + \\gamma v(S_{t+1})$ as an estimate of $q_{\\pi}(S_t, A_t)$\n",
        "* You will use $v(S_{t})$ as a baseline to reduce the variance of the updates.\n",
        "\n",
        "The function must have signature `grads = policy_gradient(net_params, obs_tm1, a_tm1, r_t, discount_t, obs_t)`.\n",
        "* Where the inputs are:\n",
        "  * the parameters `net_params` of the network,\n",
        "  * an observation `obs_tm1`\n",
        "  * the action `a_tm1` selected after observing `obs_tm1`,\n",
        "  * the resulting reward `r_t` and environment discount `discount_t` \n",
        "  * and the following observation `obs_t`\n",
        "* Returns a stochastic estimate of the policy gradient.\n",
        "  * `grads` has the same structure as `net_params`\n",
        "  * as it contains an estimate of the gradient of the expected episodic return wrt to each parameter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AHLGjy8he6vh",
        "colab": {}
      },
      "source": [
        "def policy_gradient(net_params, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5XgjefrBlIGN"
      },
      "source": [
        "### Q 3.4 [5 marks]\n",
        "\n",
        "Implement a function to compute a stochastic estimate of the negative gradient of the value loss:   $ L(\\theta) = E_{s \\sim \\pi}[(v_{\\theta}(s) - v_{\\pi}(s))^2]$\n",
        "\n",
        "As for the policy gradient, this must be computed from a 1 step transition in the environment, so using bootstrapping after one step.\n",
        "\n",
        "It must have signature `neg_grads = neg_value_loss_gradient(net_params, obs_tm1, a_tm1, r_t, discount_t, obs_t)`.\n",
        "* Where the inputs are:\n",
        "  * the current parameters `net_params` of the network,\n",
        "  * an observation `obs_tm1`\n",
        "  * the action `a_tm1` selected after observing `obs_tm1`,\n",
        "  * the resulting reward `r_t` and environment discount `discount_t` \n",
        "  * and the following observation `obs_t`\n",
        "* Returns a stochastic estimate of the policy gradient.\n",
        "  * `neg_grads` has the same structure as `net_params`\n",
        "  * as it contains a stochastic estimate of the negative gradient of the expected value prediction loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GTgSjuK0Jyid",
        "colab": {}
      },
      "source": [
        "def neg_value_loss_gradient(net_params, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jAXgLZg8mfMd"
      },
      "source": [
        "### Updating shared parameters\n",
        "\n",
        "The policy gradient identifies the direction of change in the parameters that most steeply improve the policy.\n",
        "The negative gradient of the value loss identifies the direction of change in the parameters that most steeply improves the value predictions.\n",
        "\n",
        "However, note that the value and policy share some of the parameters of the network.  How do we combine the two gradient updates?\n",
        "\n",
        "In this assignment, we will simply sum the policy and value components.\n",
        "The function that combines the two gradients is implemented for you in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uhKCLe8jjkdZ",
        "colab": {}
      },
      "source": [
        "@jax.jit\n",
        "def compute_gradient(net_params, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "  pgrads = policy_gradient(net_params, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "  vgrads = neg_value_loss_gradient(net_params, obs_tm1, a_tm1, r_t, discount_t, obs_t)\n",
        "  return jax.tree_multimap(lambda pg, vg: pg + vg, pgrads, vgrads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tYaVb6GcpCRe"
      },
      "source": [
        "### Optimisation\n",
        "\n",
        "The gradient updates are typically rescaled to avoid taking too large a step on a single update.\n",
        "For instance given a candidate gradient update $\\nabla$ we may update our parameters $\\theta$ by;\n",
        "$$\\Delta \\theta = \\theta + \\alpha * \\nabla\\,,$$\n",
        "where $\\alpha$ is a small number between 0 and 1 (e.g., $\\alpha=0.01$ or $\\alpha=0.001$), referred to as `step_size` or `learning_rate`\n",
        "\n",
        "The gradients with respect to each weight of a neural network may however have very different magnitudes. This can make it hard to set a suitable learning rate $\\alpha$.\n",
        "\n",
        "In deep learning, and deep RL, we typically use adaptive learning rates, for instance by, rescaling each component of the gradient using statistics tracking the typical size of the updates to that weight. Then the entire update is rescaled using a global `learning_rate` $\\alpha$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hbFePWfApaMU"
      },
      "source": [
        "### Q 3.5 [5 marks]\n",
        "\n",
        "A popular approach to adaptive gradient rescaling was introduced by the `Adam` algorithm.\n",
        "* See [Kingma et al, 2014](https://arxiv.org/abs/1412.6980) for references.\n",
        "\n",
        "This algorithm implements the following procedure before applying each update:\n",
        "* Increase an update counter $k \\gets k+1$ (starting at k=0 before any updates),\n",
        "* Update the first moment of each gradient component $\\mu \\gets (1 - \\eta) g + \\eta \\mu$ where $g$ is the latest stochastic gradient.\n",
        "* Update the second moment of each gradient component $\\nu_i = (1 - \\eta) g_i ^ 2 + \\eta \\nu_i $ where $g$ is the latest gradient update.\n",
        "* Use the following update to update the weights:\n",
        "$$\\Delta w = \\alpha \\frac{\\mu / (1 - \\beta_1 ^ {k})}{\\epsilon + \\sqrt{\\nu / (1 - \\beta_1 ^ {k})}}$$\n",
        "* $\\alpha$ is a global `learning rate`\n",
        "* $\\beta_1$ and $\\beta_2$ define a soft horizon for the per-weight statistics.\n",
        "* $\\epsilon$ makes the rescaling more robust to numerical issues.\n",
        "\n",
        "In the next cell define a pair of functions (`opt_init`, and `opt_update`), where:\n",
        "\n",
        "The `opt_init` function has signature `state = opt_init(params)`.\n",
        "* Takes the network parameters as inputs\n",
        "* Initialises an `optimiser state` holding the per weight statistics.\n",
        "\n",
        "The `opt_update` function has signature `updates, state = opt_update(grads, state)`.\n",
        "* Takes a `gradient` and an `optimisers state`,\n",
        "* and returns the transformed gradient and the updated `optimiser state`.\n",
        "\n",
        "Set the algorrithm's hyper-parameters to:\n",
        "* $\\beta_1=.9$ and $\\beta_2=.999$, $\\epsilon=1e-8$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "017XRA_BpbZx",
        "colab": {}
      },
      "source": [
        "opt_init, opt_update = ..., ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DczWvZfNSnTj"
      },
      "source": [
        "### Run experiments\n",
        "\n",
        "Run the cell below to show the performance of the resulting agent.\n",
        "\n",
        "You may also use this section for debugging your implementations.\n",
        "\n",
        "Note however, that most functions are `jitted` for performance,\n",
        "* either using the `@jax.jit` decorator in the function definition\n",
        "* or calling explicitely `fn = jax.jit(fn)`\n",
        "\n",
        "When jitting, the code is compiled on the first time the function is executed\n",
        "* and execution is much faster on seubsequent calls.\n",
        "* a notable side effect is that print statements in a jitted function will only execute on the first execution of the function.\n",
        "* to drop into a debugger or print on each function execution you will have to disable the `@jax.jit` annotations and jax.jit calls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fz837XTkLxE8",
        "colab": {}
      },
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "\n",
        "# Experiment configs.\n",
        "train_episodes = 1500\n",
        "discount_factor = .99\n",
        "\n",
        "# Create environment.\n",
        "env = catch.Catch(seed=42)\n",
        "\n",
        "# Build and initialize network.\n",
        "rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n",
        "sample_input = env.observation_spec().generate_value()\n",
        "net_params = init_net(next(rng), sample_input)\n",
        "\n",
        "# Initialize optimizer state.\n",
        "opt_state = opt_init(net_params)\n",
        "\n",
        "# Jit.\n",
        "opt_update = jax.jit(opt_update)\n",
        "apply_updates = jax.jit(optix.apply_updates)\n",
        "\n",
        "print(f\"Training agent for {train_episodes} episodes...\")\n",
        "all_episode_returns = []\n",
        "\n",
        "for _ in range(train_episodes):\n",
        "  episode_return = 0.\n",
        "  timestep = env.reset()\n",
        "  obs_tm1 = timestep.observation\n",
        "\n",
        "  # Sample initial action.\n",
        "  a_tm1 = policy(net_params, next(rng), obs_tm1)\n",
        "\n",
        "  while not timestep.last():\n",
        "    # Step environment.\n",
        "    new_timestep = env.step(int(a_tm1))\n",
        "\n",
        "    # Sample action from agent policy.\n",
        "    a_t = policy(net_params, next(rng), new_timestep.observation)\n",
        "\n",
        "    # Update params.\n",
        "    r_t = new_timestep.reward\n",
        "    discount_t = discount_factor * new_timestep.discount\n",
        "    dJ_dtheta = compute_gradient(\n",
        "        net_params, obs_tm1, a_tm1, r_t, discount_t, new_timestep.observation)\n",
        "    updates, opt_state = opt_update(dJ_dtheta, opt_state)\n",
        "    net_params = apply_updates(net_params, updates)\n",
        "\n",
        "    # Within episode book-keeping.\n",
        "    episode_return += new_timestep.reward\n",
        "    timestep = new_timestep\n",
        "    obs_tm1 = new_timestep.observation\n",
        "    a_tm1 = a_t\n",
        "\n",
        "  # Experiment results tracking.\n",
        "  all_episode_returns.append(episode_return)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "diKy01y1C0fz",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "def moving_average(x, w):\n",
        "    return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "smoothed_returns = moving_average(all_episode_returns, 30)\n",
        "plt.plot(smoothed_returns)\n",
        "\n",
        "plt.xlabel('Average episode returns')\n",
        "plt.xlabel('Number of episodes')\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.spines['left'].set_visible(True)\n",
        "ax.spines['bottom'].set_visible(True)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.xaxis.set_ticks_position('bottom')\n",
        "ax.yaxis.set_ticks_position('left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mh918hc0_iQ9"
      },
      "source": [
        "# Part 3: Off-policy multi-step learning [25 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "thswfgXU_p05"
      },
      "source": [
        "For many concrete algorithms, we need to combine multi-step updates with off-policy corrections.  The multi-step updates are necesary for efficient learning, while the off-policy corrections are necessary to learn about multiple things at once, or to correct for a distribution mismatch (e.g., when trying to perform a policy-gradient update from logged data).\n",
        "\n",
        "In this section, you will implement various different returns with off-policy corrections.  The next cell has two examples *without* corrections.  These examples compute equivalent returns, but compute those returns in different ways.  These are provided as reference implementations to help you.\n",
        "\n",
        "Note that the implementations both allow for immediate bootstrapping on the current state value. This is unconventional (most literature only allows the first bootstrapping to happen after the first step), but we will use this convention in all implementations below for consistency. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KHNH35SZYHBu",
        "colab": {}
      },
      "source": [
        "#@title On-policy return computations\n",
        "\n",
        "def on_policy(observations, actions, pi, mu, rewards, discounts,\n",
        "              trace_parameter, v_fn):\n",
        "  \"\"\"Compute on-policy return recursively.\"\"\"\n",
        "  del mu  # The policy probabilities are ignored by this function\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  l = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  G = np.zeros((T,))\n",
        "  # recurse backwards to calculate returns\n",
        "  for t in reversed(range(T)):\n",
        "    # There are T+1 observations, but only T rewards, and the indexing here\n",
        "    # for the rewards is off by one compared to the indexing in the slides\n",
        "    # and in Sutton & Barto.  In other words, r[t] == R_{t+1}.\n",
        "    if t == T - 1:\n",
        "      G[t] = r[t] + d[t]*v[t + 1]\n",
        "    else:\n",
        "      G[t] = r[t] + d[t]*((1 - l)*v[t + 1] + l*G[t + 1])\n",
        "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
        "  return (1 - l)*v + l*G\n",
        "\n",
        "def on_policy_error_recursion(observations, actions, pi, mu, rewards, discounts,\n",
        "                              trace_parameter, v_fn):\n",
        "  del pi  # The target policy probabilities are ignored by this function\n",
        "  del mu  # The behaviour policy probabilities are ignored by this function\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  l = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  errors = np.zeros((T,))\n",
        "  error = 0.\n",
        "  # recurse backwards to calculate errors\n",
        "  for t in reversed(range(T)):\n",
        "    error = r[t] + d[t]*v[t + 1] - v[t] + d[t]*l*error\n",
        "    errors[t] = error\n",
        "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
        "  return v + l*errors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UNXhobrYHeiy"
      },
      "source": [
        "### Q 4.1 [15 marks]\n",
        "Implement the return functions below and run the cells below that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7g6FOZLfA_su",
        "colab": {}
      },
      "source": [
        "def full_importance_sampling(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
        "  \"\"\"\n",
        "  Compute off-policy return with full importance-sampling corrections, so that\n",
        "  the return G_t is corrected with the full importance-sampling correction of\n",
        "  the rest of the trajectory.\n",
        "  \"\"\"\n",
        "\n",
        "def per_decision(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
        "  \"\"\"\n",
        "  Compute off-policy return with per-decision importance-sampling corrections.\n",
        "  \"\"\"\n",
        "\n",
        "def control_variates(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
        "  \"\"\"\n",
        "  Compute off-policy return with \n",
        "  1. per-decision importance-sampling corrections, and\n",
        "  2. control variates\n",
        "  \"\"\"\n",
        "\n",
        "def adaptive_bootstrapping(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
        "  \"\"\"\n",
        "  Compute off-policy return with \n",
        "  1. per-decision importance-sampling corrections, and\n",
        "  2. control variates, and\n",
        "  3. adaptive bootstrapping.\n",
        "\n",
        "  Implement the adaptive bootstrapping with an *additional* trace parameter\n",
        "  lambda, such that lambda_t = lambda * min(1, 1/rho_t).\n",
        "  \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "7EEHYK89ANIA",
        "colab": {}
      },
      "source": [
        "#@title (Run, don't modify) Functions to generate experience, compute values\n",
        "MU_RIGHT = 0.5\n",
        "PI_RIGHT = 0.9\n",
        "NUMBER_OF_STEPS = 5\n",
        "DISCOUNT = 0.99\n",
        "\n",
        "def generate_experience():\n",
        "  r\"\"\"Generate experience trajectories from a tabular tree MDP.\n",
        "\n",
        "  This function will start in state 0, and will then generate actions according\n",
        "  to a uniformly random behaviour policy.  When A_t == 0, the action will be to\n",
        "  the left, with A_t==1, it will be to the right.  The states are nunmbered as\n",
        "  depicted below:\n",
        "          0\n",
        "         / \\\n",
        "        1   2\n",
        "       / \\ / \\\n",
        "      3   4   5\n",
        "         ...\n",
        "  \n",
        "  Args:\n",
        "      number_of_steps: the number of total steps.\n",
        "      p_right: probability of the behaviour to go right.\n",
        "\n",
        "  Returns:\n",
        "      A dictionary with elements:\n",
        "        * observations (number_of_steps + 1 integers): the\n",
        "          observations are just the actual (integer) states\n",
        "        * actions (number_of_steps integers): actions per step\n",
        "        * rewards (number_of_steps scalars): rewards per step\n",
        "        * discounts (number_of_steps scalars): currently always 0.9,\n",
        "          except the last one which is zero\n",
        "        * mu (number_of_steps scalars): probability of selecting each\n",
        "          action according to the behavious policy\n",
        "        * pi (number_of_steps scalars): probability of selecting each\n",
        "          action according to the target policy (here p(1) = 0.9 and\n",
        "          p(0) = 0.1, where a==1 implies we go 'right')\n",
        "  \"\"\"\n",
        "  # generate actions\n",
        "  actions = np.array(np.random.random(NUMBER_OF_STEPS,) < MU_RIGHT,\n",
        "                     dtype=np.int)\n",
        "  s = 0\n",
        "  # compute resulting states\n",
        "  states = np.cumsum(np.arange(1, NUMBER_OF_STEPS + 1) + actions)\n",
        "  states = np.array([0] + list(states))  # add start state\n",
        "  # in this case, observations are just the real states\n",
        "  observations = states\n",
        "  # generate rewards\n",
        "  rewards = 2.*actions - 1. # -1 for left, +1 for right, \n",
        "  rewards[-1] = np.sum(actions)  # extra final reward for going right\n",
        "  # compute discounts\n",
        "  discounts = DISCOUNT * np.ones_like(rewards)\n",
        "  discounts[-1] = 0.  # final transition is terminal, has discount=0\n",
        "  # determine target and behaviour probabilities for the selected actions\n",
        "  pi = np.array([1. - PI_RIGHT, PI_RIGHT])[actions] # Target probabilities\n",
        "  mu = np.array([1. - MU_RIGHT, MU_RIGHT])[actions] # Behaviour probabilities\n",
        "  return dict(observations=observations,\n",
        "              actions=actions,\n",
        "              pi=pi,\n",
        "              mu=mu,\n",
        "              rewards=rewards,\n",
        "              discounts=discounts)\n",
        "\n",
        "def true_v(s, pi, number_of_steps):\n",
        "  \"\"\"Compute true state value recursively.\"\"\"\n",
        "  depth = int(np.floor((np.sqrt(1 + 8*s) - 1)/2))\n",
        "  position = int(s - depth*(depth+1)/2)\n",
        "  remaining_steps = number_of_steps - depth\n",
        "  final_reward = DISCOUNT**(remaining_steps-1)*(position + pi*remaining_steps)\n",
        "  reward_per_step = pi*(+1) + (1 - pi)*(-1)\n",
        "  discounted_steps = (1 - DISCOUNT**(remaining_steps - 1))/(1 - DISCOUNT)\n",
        "  reward_along_the_way = reward_per_step * discounted_steps\n",
        "  return reward_along_the_way + final_reward\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "CCFMUmOfRTqZ",
        "colab": {}
      },
      "source": [
        "#@title Run experiment (don't modify)\n",
        "algs = ['on_policy', 'full_importance_sampling', 'per_decision',\n",
        "        'control_variates', 'adaptive_bootstrapping']\n",
        "\n",
        "# Precompute state values (for efficiency)\n",
        "N = NUMBER_OF_STEPS\n",
        "true_vs = [true_v(s, PI_RIGHT, N) for s in range((N+1)*(N+2)//2)]\n",
        "\n",
        "def random_v(iteration, s):\n",
        "  rng = np.random.RandomState(seed=s + iteration*10000)\n",
        "  return true_vs[s] + rng.normal(loc=0, scale=1.)  # Add fixed random noise \n",
        "\n",
        "def plot_errors(ax, errors):\n",
        "  errors = np.array(errors)\n",
        "  ax.violinplot(np.log10(errors), showextrema=False)\n",
        "  ax.plot(range(1, len(algs)+1), np.log10(errors).T,\n",
        "          '.', color='#667799', ms=7, alpha=0.2)\n",
        "  ax.plot(range(1, len(algs)+1), np.log10(np.mean(errors, axis=0)),\n",
        "          '.', color='#000000', ms=20)\n",
        "  ax.set_yticks(np.arange(-2, 5))\n",
        "  ax.set_yticklabels(10.**np.arange(-2, 5), fontsize=13)\n",
        "  ax.set_ylabel(\"Value error $(v(s_0) - v_{\\\\pi}(s_0))^2$\", fontsize=15)\n",
        "  ax.set_xticks(range(1, len(algs)+1))\n",
        "  ax.set_xticklabels(algs, fontsize=15, rotation=70)\n",
        "  ax.set_ylim(-1, 4)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "\n",
        "errors = []\n",
        "estimates = []\n",
        "v0 = true_vs[0]\n",
        "for iteration in range(1000):\n",
        "  errors.append([])\n",
        "  estimates.append([])\n",
        "  trajectory = generate_experience()\n",
        "  for alg in algs:\n",
        "    estimate = eval(alg)(**trajectory,\n",
        "                        v_fn=lambda s: random_v(iteration, s),\n",
        "                        trace_parameter=0.9)\n",
        "    errors[-1].append((estimate[0] - v0)**2)\n",
        "print(np.mean(errors, axis=0))\n",
        "plot_errors(plt.gca(), errors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7hlc4jctHHqv"
      },
      "source": [
        "Above, the distributions of mean squared value errors are shown, with the mean as a big black dot and the (1,000) individual return samples as small black dots.\n",
        "\n",
        "### Q 4.2 [5 marks]\n",
        "Explain the ranking in terms of value error of the different return estimates.\n",
        "\n",
        "### Q 4.3 [5 marks]\n",
        "Could there be a reason to **not** choose the best return according to this ranking when learning off-policy?  Explain your answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "69Ro9Co-Hb0A",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}