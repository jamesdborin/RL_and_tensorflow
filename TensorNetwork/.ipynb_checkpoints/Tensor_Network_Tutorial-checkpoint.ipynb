{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "F41jzPsm_9tZ",
    "outputId": "5d4b81cd-7cea-40b9-a7b5-e4655179b3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensornetwork in /home/jamie/anaconda3/envs/Cirq/lib/python3.7/site-packages (0.4.2)\n",
      "Requirement already satisfied: jax in /home/jamie/.local/lib/python3.7/site-packages (0.1.70)\n",
      "Requirement already satisfied: jaxlib in /home/jamie/.local/lib/python3.7/site-packages (0.1.48)\n",
      "Requirement already satisfied: graphviz>=0.11.1 in /home/jamie/anaconda3/envs/Cirq/lib/python3.7/site-packages (from tensornetwork) (0.14.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/jamie/anaconda3/envs/Cirq/lib/python3.7/site-packages (from tensornetwork) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jamie/anaconda3/envs/Cirq/lib/python3.7/site-packages (from tensornetwork) (1.18.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.0 in /home/jamie/.local/lib/python3.7/site-packages (from tensornetwork) (3.2.1)\n",
      "Requirement already satisfied: scipy>=1.1 in /home/jamie/anaconda3/envs/Cirq/lib/python3.7/site-packages (from tensornetwork) (1.5.0)\n",
      "Requirement already satisfied: absl-py in /home/jamie/.local/lib/python3.7/site-packages (from jax) (0.9.0)\n",
      "Requirement already satisfied: six in /home/jamie/anaconda3/envs/Cirq/lib/python3.7/site-packages (from h5py>=2.9.0->tensornetwork) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensornetwork jax jaxlib\n",
    "import numpy as np\n",
    "import jax\n",
    "import tensornetwork as tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaWIxu_4aBNj"
   },
   "source": [
    "# Section 1: Basic usage.\n",
    "In this section, we will go over basic linear algebra operations and how to create them using a TensorNetwork. While at first it may seem more complicated to use a tensornetwork rather than just doing the operations by hand, we will use the skills developed in this section to start building and contracting very complicated tensor networks that would be very difficult to do otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4i5M1zLbbsL"
   },
   "source": [
    "Let's begin by doing the most basic operation possible, a vector dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "R_9ZTrSoAah3",
    "outputId": "fcc9c983-29b0-4a95-ce85-7f3e34e7645d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# Next, we add the nodes containing our vectors.\n",
    "a = tn.Node(np.ones(10))\n",
    "# Either tensorflow tensors or numpy arrays are fine.\n",
    "b = tn.Node(np.ones(10))\n",
    "# We \"connect\" these two nodes by their \"0th\" edges.\n",
    "# This line is equal to doing `tn.connect(a[0], b[0])\n",
    "# but doing it this way is much shorter.\n",
    "edge = a[0] ^ b[0]\n",
    "# Finally, we contract the edge, giving us our new node with a tensor\n",
    "# equal to the inner product of the two earlier vectors\n",
    "c = tn.contract(edge)\n",
    "# You can access the underlying tensor of the node via `node.tensor`.\n",
    "# To convert a Eager mode tensorflow tensor into \n",
    "print(c.tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gf_tCdk7lNeN"
   },
   "source": [
    "## Edge-centric connection.\n",
    "When a node is created in the TensorNetwork, that node is automatically filled with dangling-edges. To connect two nodes together, we actually remove the two danging edges in the nodes and replace them with a standard/trace edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "cLuYsq_clLTA",
    "outputId": "5a59d282-5b50-4991-c84e-f3b0524c2e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of a[0] is: <class 'tensornetwork.network_components.Edge'>\n",
      "Is a[0] dangling?: True\n"
     ]
    }
   ],
   "source": [
    "a = tn.Node(np.eye(2))\n",
    "# Notice that a[0] is actually an \"Edge\" type.\n",
    "print(\"The type of a[0] is:\", type(a[0]))\n",
    "# This is a dangling edge, so this method will \n",
    "print(\"Is a[0] dangling?:\", a[0].is_dangling())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySDV2F3lm5_S"
   },
   "source": [
    "Now, let's connect a[0] to a[1]. This will create a \"trace\" edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "L1BBz3Zymuea",
    "outputId": "ab473c27-a492-4a9a-9b6b-038c3d0faca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are a[0] and a[1] the same edge?: True\n",
      "Is a[0] dangling?: False\n"
     ]
    }
   ],
   "source": [
    "trace_edge = a[0] ^ a[1]\n",
    "# Notice now that a[0] and a[1] are actually the same edge.\n",
    "print(\"Are a[0] and a[1] the same edge?:\", a[0] is a[1])\n",
    "print(\"Is a[0] dangling?:\", a[0].is_dangling())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cW-IWA_l0By"
   },
   "source": [
    "## Axis naming.\n",
    "Sometimes, using the axis number is very inconvient and it can be hard to keep track of the purpose of certain edges. To make it easier, you can optionally add a name to each of the axes of your node. Then you can get the respective edge by indexing using the name instead of the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "M0k91fG9A-nc",
    "outputId": "61aebd0c-f0b9-4f2c-91b1-8da39c1c9fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# Here, a[0] is a['alpha'] and a[1] is a['beta']\n",
    "a = tn.Node(np.eye(2), axis_names=['alpha', 'beta'])\n",
    "edge = a['alpha'] ^ a['beta']\n",
    "result = tn.contract(edge)\n",
    "print(result.tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srvhkdXyq93b"
   },
   "source": [
    "# Section 2. Advanced Network Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgEhqwUV0tmd"
   },
   "source": [
    "## Avoid trace edges.\n",
    "While the TensorNetwork library fully supports trace edges, contraction time is ALWAYS faster if you avoid creating them. This is because trace edges only sum the diagonal of the underlying matrix, and the rest of the values (which is a majorit of the total values) are just garbage. You both waste compute time and memory by having these useless trace edges.\n",
    "\n",
    "The main way we support avoid trace edges is via the `@` symbol, which is an alias to `tn.contract_between`. Take a look at the speedups!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "nqlIH0wOqvd0",
    "outputId": "bebafd5a-b435-4a0d-ae8e-4201149b260f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running one_edge_at_a_time\n",
      "1 loop, best of 3: 120 ms per loop\n",
      "Running use_cotract_between\n",
      "10 loops, best of 3: 54.4 ms per loop\n"
     ]
    }
   ],
   "source": [
    "def one_edge_at_a_time(a, b):\n",
    "  node1 = tn.Node(a)\n",
    "  node2 = tn.Node(b)\n",
    "  edge1 = node1[0] ^ node2[0]\n",
    "  edge2 = node1[1] ^ node2[1]\n",
    "  tn.contract(edge1)\n",
    "  result = tn.contract(edge2)\n",
    "  return result.tensor\n",
    "\n",
    "def use_contract_between(a, b):\n",
    "  node1 = tn.Node(a)\n",
    "  node2 = tn.Node(b)\n",
    "  node1[0] ^ node2[0]\n",
    "  node1[1] ^ node2[1]\n",
    "  # This is the same as \n",
    "  # tn.contract_between(node1, node2)\n",
    "  result = node1 @ node2\n",
    "  return result.tensor\n",
    "\n",
    "a = np.ones((1000, 1000))\n",
    "b = np.ones((1000, 1000))\n",
    "print(\"Running one_edge_at_a_time\")\n",
    "%timeit one_edge_at_a_time(a, b)\n",
    "print(\"Running use_cotract_between\")\n",
    "%timeit use_contract_between(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFgUTwnt3Umg"
   },
   "source": [
    "We also have `contract_parallel` which does the same thing as `contract_between`, only you pass a single edge instead of two nodes. This will contract all of the edges \"parallel\" to the given edge (meaning all of the edges that share the same two nodes as the given edge).\n",
    "\n",
    "Using either method is fine and they will do the exact same thing. In fact, if you look at the source code, `contract_parallel` actually just calls `contract_between`. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "aYXoLPNA22fm",
    "outputId": "c0786726-53a9-41f2-dc40-617c73cf166b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running contract_parallel\n",
      "10 loops, best of 3: 53.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "def use_contract_parallel(a, b):\n",
    "  node1 = tn.Node(a)\n",
    "  node2 = tn.Node(b)\n",
    "  edge = node1[0] ^ node2[0]\n",
    "  node1[1] ^ node2[1]\n",
    "  result = tn.contract_parallel(edge)\n",
    "  # You can use `get_final_node` to make sure your network \n",
    "  # is fully contracted.\n",
    "  return result.tensor\n",
    "\n",
    "print(\"Running contract_parallel\")\n",
    "%timeit use_contract_parallel(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyUb_CyaZqmv"
   },
   "source": [
    "## Complex Contraction.\n",
    "Remember this crazy hard to write tensor contraction?\n",
    "Well, we're gonna do it in about 13 lines of simple code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "OzAlIQ9eyv8Z",
    "outputId": "ce5a2023-71ae-47cf-f842-29669fdbacbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.0\n"
     ]
    }
   ],
   "source": [
    "# Here, we will contract the following shaped network.\n",
    "# O - O\n",
    "# | X |\n",
    "# O - O\n",
    "a = tn.Node(np.ones((2, 2, 2)))\n",
    "b = tn.Node(np.ones((2, 2, 2)))\n",
    "c = tn.Node(np.ones((2, 2, 2)))\n",
    "d = tn.Node(np.ones((2, 2, 2)))\n",
    "# Make the network fully connected.\n",
    "a[0] ^ b[0]\n",
    "a[1] ^ c[1]\n",
    "a[2] ^ d[2]\n",
    "b[1] ^ d[1]\n",
    "b[2] ^ c[2]\n",
    "c[0] ^ d[0]\n",
    "# We are using the \"greedy\" contraction algorithm.\n",
    "# Other algorithms we support include \"optimal\" and \"branch\".\n",
    "\n",
    "# Finding the optimial contraction order in the general case is NP-Hard,\n",
    "# so there is no single algorithm that will work for every tensor network.\n",
    "# However, there are certain kinds of networks that have nice properties that\n",
    "# we can expliot to making finding a good contraction order easier.\n",
    "# These types of contraction algorithms are in developement, and we welcome \n",
    "# PRs!\n",
    "\n",
    "# `tn.reachable` will do a BFS to get all of the nodes reachable from a given\n",
    "# node or set of nodes.\n",
    "# nodes = {a, b, c, d}\n",
    "nodes = tn.reachable(a)\n",
    "result = tn.contractors.greedy(nodes)\n",
    "print(result.tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "6xZybTdRVaeq",
    "outputId": "3babe82d-3251-4285-aebe-7b76c14b119f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensornetwork/ncon_interface.py:130: UserWarning: Suboptimal ordering detected. Edges ['4'] are not adjacent in the contraction order to edges ['2'], connecting nodes ['con(tensor_0,tensor_1)', 'tensor_2']. Deviating from the specified ordering!\n",
      "  list(map(str, nodes_to_contract))))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(64.)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To make connecting a network a little less verbose, we have included\n",
    "# the NCon API aswell.\n",
    "\n",
    "# This example is the same as above.\n",
    "ones = np.ones((2, 2, 2))\n",
    "tn.ncon([ones, ones, ones, ones], \n",
    "        [[1, 2, 4], \n",
    "         [1, 3, 5], \n",
    "         [2, 3, 6],\n",
    "         [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "2x1GbZcYWTrT",
    "outputId": "dbffe7a7-214f-45c5-e8d7-789b2dca5676"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2.],\n",
       "       [2., 2.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To specify dangling edges, simply use a negative number on that index.\n",
    "\n",
    "ones = np.ones((2, 2))\n",
    "tn.ncon([ones, ones], [[-1, 1], [1, -2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SQz6EzVuOnk"
   },
   "source": [
    "# Section 3: Node splitting.\n",
    "In the final part of this colab, will go over the SVD node splitting methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVOluV8-4YPD"
   },
   "outputs": [],
   "source": [
    "# To make the singular values very apparent, we will just take the SVD of a\n",
    "# diagonal matrix.\n",
    "diagonal_array = np.array([[2.0, 0.0, 0.0],\n",
    "                           [0.0, 2.5, 0.0],\n",
    "                           [0.0, 0.0, 1.5]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "DUc-1xWnwcDY",
    "outputId": "da57a616-4594-417c-94ce-ed01f5aa7668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U node\n",
      "[[0.        1.4142135 0.       ]\n",
      " [1.5811388 0.        0.       ]\n",
      " [0.        0.        1.2247449]]\n",
      "\n",
      "V* node\n",
      "[[0.        1.5811388 0.       ]\n",
      " [1.4142135 0.        0.       ]\n",
      " [0.        0.        1.2247449]]\n"
     ]
    }
   ],
   "source": [
    "# First, we will go over the simple split_node method.\n",
    "a = tn.Node(diagonal_array)\n",
    "u, vh, _ = tn.split_node(\n",
    "    a, left_edges=[a[0]], right_edges=[a[1]])\n",
    "print(\"U node\")\n",
    "print(u.tensor)\n",
    "print()\n",
    "print(\"V* node\")\n",
    "print(vh.tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "DGlRgZq82LP2",
    "outputId": "aa0d0859-acb6-40d2-a625-ac21e38bbc00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contraction of U and V*:\n",
      "[[1.9999999 0.        0.       ]\n",
      " [0.        2.5       0.       ]\n",
      " [0.        0.        1.5000001]]\n"
     ]
    }
   ],
   "source": [
    "# Now, we can contract u and vh to get back our original tensor!\n",
    "print(\"Contraction of U and V*:\")\n",
    "print((u @ vh).tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkBXbOC65qnj"
   },
   "source": [
    "We can also drop the lowest singular values in 2 ways, \n",
    "1. By setting `max_singular_values`. This is the maximum number of the original\n",
    "singular values that we want to keep.\n",
    "2. By setting `max_trun_error`. This is the maximum amount the sum of the removed singular values can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "jQZQn4T32YJ3",
    "outputId": "5423113b-3b06-46e5-a890-9ab3da678dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.9999999 0.        0.       ]\n",
      " [0.        2.5       0.       ]\n",
      " [0.        0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "# We can also drop the lowest singular values in 2 ways, \n",
    "# 1. By setting max_singular_values. This is the maximum number of the original\n",
    "# singular values that we want to keep.\n",
    "a = tn.Node(diagonal_array)\n",
    "u, vh, truncation_error = tn.split_node(\n",
    "    a, left_edges=[a[0]], right_edges=[a[1]], max_singular_values=2)\n",
    "# Notice how the two largest singular values (2.0 and 2.5) remain\n",
    "# but the smallest singular value (1.5) is removed.\n",
    "print((u @ vh).tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYXIXUYY3nf7"
   },
   "source": [
    "We can see the values of the removed singular values by looking at the returned `truncation_error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "PXL9HSmJ3OWe",
    "outputId": "528f4c65-ef26-4454-8fe0-728735f7b41e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5]\n"
     ]
    }
   ],
   "source": [
    "# truncation_error is just a normal tensorflow tensor.\n",
    "print(truncation_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycqQuPYSEnP2"
   },
   "source": [
    "# Section 4: running on GPUs\n",
    "\n",
    "To get this running on a GPU, we recommend using the JAX backend, as it has nearly the exact same API as numpy.\n",
    "\n",
    "To get a GPU, go to Runtime -> Change runtime type -> GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "vQKueu8aEqwM",
    "outputId": "862514f1-d44e-480c-fcc9-161cc379cedd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Backend\n",
      "1 loop, best of 3: 5.46 s per loop\n",
      "JAX Backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:115: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 2.96 s per loop\n"
     ]
    }
   ],
   "source": [
    "def calculate_abc_trace(a, b, c):\n",
    "  an = tn.Node(a)\n",
    "  bn = tn.Node(b)\n",
    "  cn = tn.Node(c)\n",
    "  an[1] ^ bn[0]\n",
    "  bn[1] ^ cn[0]\n",
    "  cn[1] ^ an[0]\n",
    "  return (an @ bn @ cn).tensor\n",
    "\n",
    "a = np.ones((4096, 4096))\n",
    "b = np.ones((4096, 4096))\n",
    "c = np.ones((4096, 4096))\n",
    "\n",
    "tn.set_default_backend(\"numpy\")\n",
    "print(\"Numpy Backend\")\n",
    "%timeit calculate_abc_trace(a, b, c)\n",
    "tn.set_default_backend(\"jax\")\n",
    "# Running with a GPU: 202 ms\n",
    "# Running with a CPU: 2960 ms\n",
    "print(\"JAX Backend\")\n",
    "%timeit np.array(calculate_abc_trace(a, b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaJpprAMWI7S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tensor Network Tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
